\documentclass[a4paper,12pt]{article}
\usepackage[a4paper, top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}

\usepackage{float}

\usepackage[style=alphabetic,sorting=nyt]{biblatex}
% setting the path to the bib file
\addbibresource{sources.bib}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{pdfpages}
\usepackage{tocloft}
\setcounter{tocdepth}{3}

\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{hyperref}
\hypersetup{
  colorlinks=false,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}
\usepackage[english]{babel}
\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage\ of \pageref{LastPage}}

\setlength{\parskip}{1em}
\setlength{\parindent}{0px}

\title{A unifying framework for quanitifying the data propogation bottlenecks of graph representation learning methods}

\author{
  \color{red}  Mustafa Hekmat Al-Abdelamir\\
  \color{red}  Joshua Victor NiemelÃ¤\\
}
\date{\today}


\begin{document}


\maketitle



\section{Introduction}

Topological Deep Learning (TDL) is gaining traction as a novel approach~\cite{papamarkou_position:_2024} for Graph Representation Learning (GRL).
Leveraging topology, TDL has shown promising results in alleviating various limitations of Graph Neural Networks (GNNs)~\cite{horn_topological_2022}.

Two often-cited related\cite{giraldo_trade-off_2023} shortcomings in GNNs are over-smoothing and over-squashing.
Over-smoothing occurs when individual node features become washed out and too similar after multiple graph convolutions~\cite{li_deeper_2018}.
In message-passing, nodes send fixed-size messages to their neighbours, said neighbours aggregate the messages, update their features, then send out new messages and so on.
This process inevitably leads to information loss, as an increasingly large amount of information is compressed into fixed-size vectors.
This is known as over-smoothing~\cite{alon_bottleneck_2021}.

Still, perhaps because of the young nature of the field, there is limited theoretical foundation for quantifying these possible advantages.
This poses a problem in making quantitative comparisons between various architectures for GRL.
In this paper, we attempt to lay the foundations for various metrics to provide insights on over-squashing and over-smoothing and how they relate to the model's ability to learn.
To verify and support our theoretical foundations, we also run benchmarks against other approaches used for learning on graph data~\cite{horn_topological_2022}

\section{Data}
We will be using the QM9 dataset~\cite{blum}\cite{rupp} as real-world data to benchmark our models against
other models from the cited papers and to test the metrics we will develop.

We will also be creating and using synthetic datasets as this will give us more control over the data and allow us to create specific topological features and properties that we can then use to test the metrics and models.

\section{Methods}
\begin{itemize}
	\item  We will be using PyTorch Geometric to build our models and to replicate models cited in other papers as a comparison and empirical exploration of GRL.
	\item Docker will be used to containerise our experiments and replications to ensure reproducibility as well as make it easier to run experiments on different machines.
\end{itemize}

\section{Learning Objectives}


\begin{enumerate}
	\item Gain a solid understanding of the theoretical foundations of GNNs.
	\item Investigate the over-squashing problem in GNNs and how it affects the ability of the model to learn long-range dependencies~\cite{alon_bottleneck_2021}.
	\item Gain insights and understanding about TDL, how topology is leveraged for learning, and how it relates to the aforementioned bottlenecks~\cite{horn_topological_2022}.
	\item Construct generalisable metrics to quantify various geometric and topological properties of GNNs and the datasets they are trained on.
	\item Construct a model, for instance, a transformer or CCNN~\cite{tdlbook}, that can learn topological features in data and benchmark against non-topological approaches.
\end{enumerate}


\section{Three-node GCN}

To dip our toes into the over-squashing problem,
we started out with a simple GCN model with three nodes and one layer.
The GCN layer has exactly one parameter which is the coeffecient in the aggregation step, we have one parameter since we have only one output feature because all node connections, including self-connections, are aggregated in the same manner.
The update and aggregate function with normalisation enabled is described below. The trainable parameter is \(\Theta\).
\begin{equation}
  \mathbf{x}_i' = \Theta \sum_{j \in \mathcal{N}(v) \cup \{i\}} \frac{e_{j,i}}{\sqrt{\hat{d}_j \hat{d}_i}} \mathbf{x}_j
\end{equation}

The formula without normalisation is identical, albeit without the diagonal degree matrix term:
\begin{equation}
  \mathbf{x}_i' = \Theta \sum_{j \in \mathcal{N}(v) \cup \{i\}} e_{j,i} \mathbf{x}_j
\end{equation}

Our dataset / problem is a graph of three nodes, with two children pointing into the root node. The root node is set to 0 (\(x_{1}=0\)) and the children have some random integer, the target is the sum of the entire graph. The readout is only done by reading the root node values.
\begin{figure}[H]
  \centering
\begin{tikzpicture}
  % Define the nodes
  \node[circle, draw] (0) at (0, 0) {$x_{1}$};
  \node[circle, draw] (1) at (-1, 1) {$x_2$};
  \node[circle, draw] (2) at (1, 1) {$x_3$};

  % Draw the arrows
  \draw[->] (1) -- (0);
  \draw[->] (2) -- (0);
\end{tikzpicture}
\end{figure}
Algebraically, this problem is solved by
\begin{equation}
x_{1}' = x_{2}+x_{3} \label{three_node_solution}
\end{equation}


We first compute what the learnable parameter should be with, and without normalisation, and then we try to train the model and verify that we get the expected weights.

Our edge weights are fixed to 1, which means all edges have equal weighting, \(e_{j, i}=1\) Since we only do one graph convolution towards the root, we can disregard the updates for the two children nodes and only look at the update that affects the root, which means we can set \(i=1\).

This gives us the simplest problem we could conceive, a linear function with one parameter:
\begin{align}
  \mathbf{x}_1' &= \Theta \sum_{j \in \mathcal{N}(v) \cup \{1\}} \mathbf{x}_j,\ \Theta \in \mathbb{R}\\
   &= \Theta (x_{1} + x_{2} + x_{3})\\
                &= \Theta (x_{2} + x_{3})\\
  \intertext{We observe that fixing \(\Theta=1\) yields eq (\ref{three_node_solution}):}
                  &= x_{2} + x_{3}
\end{align}

Next, we do the same for the normalised example, $\hat{d}_{i}$ is the number of neighbours of node $i$ plus one. This means, we have $\hat{d}_{2} = \hat{d}_{3} = 1$ and $\hat{d}_{1}=3$.
\begin{align}
  \mathbf{x}_1' &= \Theta \sum_{j \in \mathcal{N}(v) \cup \{i\}} \frac{1}{\sqrt{\hat{d}_j \hat{d}_i}} \mathbf{x}_j\\
  &= \Theta  \left( \frac{1}{\sqrt{\hat{d}_2 \hat{d}_1}} \mathbf{x}_2 +  \frac{1}{\sqrt{\hat{d}_3 \hat{d}_1}} \mathbf{x}_3 \right)\\
  &= \Theta  \left( \frac{1}{\sqrt{1 \cdot 3}} \mathbf{x}_2 +  \frac{1}{\sqrt{1 \cdot 3}} \mathbf{x}_3 \right)\\
  &= \frac{\Theta}{\sqrt{3}}  \left(  \mathbf{x}_2 +   \mathbf{x}_3 \right) \implies \theta = \sqrt{3} \approx 1.732\\
\end{align}

SHOW RESULTS HERE (
Homework for Mustafa)

Next, we tried a slightly more complex problem, again graphs of size 3 in the same shape. But this time we want to classify if $x_{1} = x_{2}+x_{3}$ is true or not for the given graph.


\section{Tree neighbours-match}
We decided to try to reproduce the over-squashing problem from \cite{alon_bottleneck_2021}.
\subsection{Data}
The dataset of a given depth $d$ is comprised of binary trees of depth $d$ and $n$ unique iid. trees. Each node of the graph has two features, a class, and the number of ``leaves''. We can represent a tree as a directed and connected graph $G \in (V, E)$ where $(i, j) \in E$ [COMMENT FROM RAGHAV?].
We let $A$ represent the adjacency matrix for a given tree without any self-loops. Let $A_{i,j}= \text{if } \lfloor \frac{i}{2}\rfloor = j \text{ then } 1 \text{ else } 0$. We use self-loops since they have a positive effect on performance [CITE THE CURVATURE PAPER(?)]: $\bar{A}=A+I$, where $I$ is the identity matrix.

The $V \in \mathbb{N}^{2 \times n}$ contain our attributes for the nodes. All nodes other than nodes at depth $d$ have a class of 0: $V_{1, i_{1}} = 0, i_{1} \in \{0, 1, 2, \ldots, n-2^{d}-1, n-2^{d}\}$. The remaining nodes are labelled in ascending order: $V_{1, i_{2}}= i_{2}-(n-2^{d}), i_{2} \in \{n-2^{d}+1, \ldots, n-1, n\}$.
The root has a random number of leaves between $1$ and $2^{d}$. The nodes in the last layer are sampled to have a random number of leaves between $1$ and $2^{d}$ without replacement. All nodes between depth $1$ to $d-1$ are set to have 0 leaves. The label for the dataset is then finally set to be the class whose leaves match the number of leaves of the root. Note that the edge matrix is 1-indexed and the attribute matrix is 0-indexed for ease of notation.

NOTE:
Our tree neighbours using the approximation fr birthday problem has almost 0 likelihood for collisions between test and train


\section{NOTES FROM RESEARCH THAT HAVE NOT BEEN ORGANISED!}

For each given depth $d$, we have $2^{d}! \cdot 2^{d}$ ($2^{d}!$ permutations of the bottom layer, $2^{d}$ possible root labels) possible trees / samples. We notice this means that for $d=2$ and $d=3$, we only get $96$ and $322560$ unique trees respectively.
We sample from this dataset by generating a binary tree, then creating a permutation of the unique number of leaves and then randomly picking a class that the root should mimic.

By the birthday paradox, we know this means these depths will very likely contain duplicate entries in the train and test data. Since they are both sampled IID this means it will not result in overfitting. In the event that the model has perfectly learnt the training data which contains all possible unique entries, we know that any future samples we throw at the model will also just contain the same entries we can classify.
For depths greater then 3, the number of unique trees grows to the point where the likelihood of duplicate entries goes towards 0.


\subsection{Running the experiment} % tree neighbours-match

Although in \cite{alon_bottleneck_2021}, the authors present their findings on the tree neighbours-match dataset, they do not provide results of implementing the fully adjacent last layer which they otherwise more widely propose as a heuristic approach to deal with over-squashing.
This has led us to implement the fully adjacent last layer and compare the model with and without the last layer. We have in total run 1135 experiments, with a random distribution of parameters, to see how the GCN models with and without the fully adjacent last layer compare. The results are presented in Figure \ref{fig:tree_experiment_graph}. More detailed results can be found \href{PLACE SOMETHING HERE MAYBE A LINK TO THE RESULTS IN THE}{here}.

\begin{figure}[H]
	\centering
	\input{tree_experiment_graph.tex}
	\caption{The results of the tree neighbours-match experiment.}
	\label{fig:tree_experiment_graph}
\end{figure}

\section{Model architecture} % tree neighbours-match

The two node features are embedded in a 32 dimensional space using a linear layer with trainable weights without a bias parameter. We used RELU as our activation function and mean as our graph convolution aggregator. The models have $d+1$ layers, where $d$ is the depth of the trees in our given dataset.
We use the [INSERT THE NAME OF THIS THING] normalisation as utilised in PyTorch Geometric. We used ADAM and a reduce LR on plateau scheduler with [PARAMS?].

The last fully adjacent layer connects every single node to the root, we can omit the remaining pairwise connections since the resulting messages don't get propagated to the root before we finish the message passing. Let $r$ be the root node, we then have: $E_{FA} \subseteq \{(i, j) \mid i \in V, j = r\}$.

\subsection{Results} % tree neighbours-match

The results of the tree neighbours-match experiment show that contrary to the findings in \cite{alon_bottleneck_2021}, the GCN with the fully adjacent last layer does not outperform the GCN model without the fully adjacent last layer. The results are consistent across all depths. This is an interesting result and motivates further investigation into the over-squashing problem on a more theoretical level rather than just heuristic, as it is difficult to draw any conclusions from our contradictory results alone.


\section{MLP aggregate on GCN}
\[
	h_v^{(k)} = \text{UPDATE}^{(k)} \left(MLP^{(k)}(\{ h_u^{(k-1)}: u \in \mathcal{N}(v) \cup v \}) \right)
\]

\printbibliography

\end{document}

\documentclass[a4paper,12pt]{article}
\usepackage[a4paper, hmargin={1.8cm, 1.8cm}, vmargin={1cm, 1cm}]{geometry}

\usepackage[style=alphabetic,sorting=nyt]{biblatex}
\addbibresource{sources.bib}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{tocloft}
\setcounter{tocdepth}{3}

\usepackage{hyperref}
\hypersetup{
  colorlinks=false,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}
\usepackage[english]{babel}


\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage\ of \pageref{LastPage}}

\setlength{\parskip}{1em}
\setlength{\parindent}{0px}

\title{A unifying framework for quanitifying the data propogation bottlenecks of graph representation learning methods}

\author{
  \color{red}  Mustafa Hekmat Al-Abdelamir\\
  \color{red}  Joshua Victor Niemel√§\\
}
\date{\today}


\begin{document}


\maketitle



\section{Introduction}

Topological Deep Learning (TDL) is gaining traction as a novel approach~\cite{papamarkou_position:_2024} for Graph Representation Learning (GRL).
Leveraging topology, TDL has shown promising results in alleviating various limitations of Graph Neural Networks (GNNs)~\cite{horn_topological_2022}.

Two often-cited related\cite{giraldo_trade-off_2023} shortcomings in GNNs are over-smoothing and over-squashing.
Over-smoothing occurs when individual node features become washed out and too similar after multiple graph convolutions~\cite{li_deeper_2018}.
In message-passing, nodes send fixed-size messages to their neighbours, said neighbours aggregate the messages, update their features, then send out new messages and so on.
This process inevitably leads to information loss, as an increasingly large amount of information is compressed into fixed-size vectors.
This is known as over-smoothing~\cite{alon_bottleneck_2021}.

Still, perhaps because of the young nature of the field, there is limited theoretical foundation for quantifying these possible advantages.
This poses a problem in making quantitative comparisons between various architectures for GRL.
In this paper, we attempt to lay the foundations for various metrics to provide insights on over-squashing and over-smoothing and how they relate to the model's ability to learn.
To verify and support our theoretical foundations, we also run benchmarks against other approaches used for learning on graph data~\cite{horn_topological_2022}

\section{Data}
We will be using the QM9 dataset~\cite{blum}\cite{rupp} as real-world data to benchmark our models against
other models from the cited papers and to test the metrics we will develop.

We will also be creating and using synthetic datasets as this will give us more control over the data and allow us to create specific topological features and properties that we can then use to test the metrics and models.

\section{Methods}
\begin{itemize}
	\item  We will be using PyTorch Geometric to build our models and to replicate models cited in other papers as a comparison and empirical exploration of GRL.
	\item Docker will be used to containerise our experiments and replications to ensure reproducibility as well as make it easier to run experiments on different machines.
\end{itemize}

\section{Learning Objectives}


\begin{enumerate}
	\item Gain a solid understanding of the theoretical foundations of GNNs.
	\item Investigate the over-squashing problem in GNNs and how it affects the ability of the model to learn long-range dependencies~\cite{alon_bottleneck_2021}.
	\item Gain insights and understanding about TDL, how topology is leveraged for learning, and how it relates to the aforementioned bottlenecks~\cite{horn_topological_2022}.
	\item Construct generalisable metrics to quantify various geometric and topological properties of GNNs and the datasets they are trained on.
	\item Construct a model, for instance, a transformer or CCNN~\cite{tdlbook}, that can learn topological features in data and benchmark against non-topological approaches.
\end{enumerate}

\section{Tree neighbours-match}
We decided to try to reproduce the over-squashing problem from \cite{alon_bottleneck_2021}.
\subsection{Data}
The dataset of a given depth $d$ is comprised of binary trees of depth $d$ and $n$ unique iid. trees. Each node of the graph has two attributes, a class, and the number of ``leaves''. We can represent a tree as a graph $G \in (V, E)$. We can construct the edge list, $E \in \mathbb{N}^{2 \times (2n-1)}$ in two parts. The first half contains all the edges going from the children towards the root. We have $E_{1, i_{1}}=i_{1}$ and $E_{2, i_{1}} = \lfloor \frac{i_{1}-1}{2} \rfloor$ for $i_{1} \in \{1, 2, \ldots, n-1\}$. The second part contains self-loops between all the nodes in the graph, $E_{1, i_{2}} = E_{2, i_{2}}=i_{2}-n$ for $i_{2} \in \{n, n+1, \ldots, 2n-1, 2n\}$.

The $V \in \mathbb{N}^{2 \times n}$ contain our attributes for the nodes. All nodes other than nodes at depth $d$ have a class of 0: $V_{1, i_{1}} = 0, i_{1} \in \{0, 1, 2, \ldots, n-2^{d}-1, n-2^{d}\}$. The remaining nodes are labelled in ascending order: $V_{1, i_{2}}= i_{2}-(n-2^{d}), i_{2} \in \{n-2^{d}+1, \ldots, n-1, n\}$.
The root has a random number of leaves between $1$ and $2^{d}$. The nodes in the last layer are sampled to have a random number of leaves between $1$ and $2^{d}$ without replacement. All nodes between depth $1$ to $d-1$ are set to have 0 leaves. The label for the dataset is then finally set to be the class whose leaves match the number of leaves of the root.

NOTE:
Our tree neighbours using the approximation fr birthday problem has almost 0 likelihood for collisions between test and train

both test and train are by definition IID but for depth 2 and 3 theres a ~100\% likelihood of samples existing on both sides


\section{NOTES FROM RESEARCH THAT HAVE NOT BEEN ORGANISED!}

We reproduced the ``tree neighbours-match'' dataset from BOTTLENECK SOURCE.
The dataset is a made of full balanced binary trees. Each node has two values, a class and a number of ``leaves''. All nodes that aren't part of the bottom layer have a class of 0 or ``unspecified''. Each node in the bottom layer are labelled incrementally, so the leftmost child is class 1 and the rightmost child is $2^{d}$ and a unique number of leaves. The root has an unspecified class and a unique number of leaves that corresponds to one of the bottom nodes. The task is then for the network to figure out what class the root pairs up from the bottom layer.

These balanced binary trees are directed graphs pointing up towards the root.

For each given depth $d$, we have $2^{d}! \cdot 2^{d}$ ($2^{d}!$ permutations of the bottom layer, $2^{d}$ possible root labels) possible trees / samples. We notice this means that for $d=2$ and $d=3$, we only get $96$ and $322560$ unique trees respectively.
We sample from this dataset by generating a binary tree, then creating a permutation of the unique number of leaves and then randomly picking a class that the root should mimic.

By the birthday paradox, we know this means these depths will very likely contain duplicate entries in the train and test data. Since they are both sampled IID this means it will not result in overfitting. In the event that the model has perfectly learnt the training data which contains all possible unique entries, we know that any future samples we throw at the model will also just contain the same entries we can classify.
For depths greater then 3, the number of unique trees grows to the point where the likelihood of duplicate entries goes towards 0.

\printbibliography

\end{document}

\documentclass[a4paper,12pt]{article}
\usepackage[a4paper, top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}

\usepackage{float}

\usepackage[style=alphabetic,sorting=nyt]{biblatex}
% setting the path to the bib file
\addbibresource{sources.bib}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{pdfpages}
\usepackage{tocloft}
\setcounter{tocdepth}{3}

\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{hyperref}
\hypersetup{
  colorlinks=false,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}
\usepackage[english]{babel}
\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage\ of \pageref{LastPage}}

\setlength{\parskip}{1em}
\setlength{\parindent}{0px}

\title{A unifying framework for quanitifying the data propogation bottlenecks of graph representation learning methods}

\author{
  \color{red}  Mustafa Hekmat Al-Abdelamir\\
  \color{red}  Joshua Victor Niemel√§\\
}
\date{\today}


\begin{document}


\maketitle



\section{Introduction}

Topological Deep Learning (TDL) is gaining traction as a novel approach~\cite{papamarkou_position:_2024} for Graph Representation Learning (GRL).
Leveraging topology, TDL has shown promising results in alleviating various limitations of Graph Neural Networks (GNNs)~\cite{horn_topological_2022}.

Two often-cited related\cite{giraldo_trade-off_2023} shortcomings in GNNs are over-smoothing and over-squashing.
Over-smoothing occurs when individual node features become washed out and too similar after multiple graph convolutions~\cite{li_deeper_2018}.
In message-passing, nodes send fixed-size messages to their neighbours, said neighbours aggregate the messages, update their features, then send out new messages and so on.
This process inevitably leads to information loss, as an increasingly large amount of information is compressed into fixed-size vectors.
This is known as over-smoothing~\cite{alon_bottleneck_2021}.

Still, perhaps because of the young nature of the field, there is limited theoretical foundation for quantifying these possible advantages.
This poses a problem in making quantitative comparisons between various architectures for GRL.
In this paper, we attempt to lay the foundations for various metrics to provide insights on over-squashing and over-smoothing and how they relate to the model's ability to learn.
To verify and support our theoretical foundations, we also run benchmarks against other approaches used for learning on graph data~\cite{horn_topological_2022}

\section{Data}
We will be using the QM9 dataset~\cite{blum}\cite{rupp} as real-world data to benchmark our models against
other models from the cited papers and to test the metrics we will develop.

We will also be creating and using synthetic datasets as this will give us more control over the data and allow us to create specific topological features and properties that we can then use to test the metrics and models.

\section{Methods}
\begin{itemize}
	\item  We will be using PyTorch Geometric to build our models and to replicate models cited in other papers as a comparison and empirical exploration of GRL.
	\item Docker will be used to containerise our experiments and replications to ensure reproducibility as well as make it easier to run experiments on different machines.
\end{itemize}

\section{Learning Objectives}


\begin{enumerate}
	\item Gain a solid understanding of the theoretical foundations of GNNs.
	\item Investigate the over-squashing problem in GNNs and how it affects the ability of the model to learn long-range dependencies~\cite{alon_bottleneck_2021}.
	\item Gain insights and understanding about TDL, how topology is leveraged for learning, and how it relates to the aforementioned bottlenecks~\cite{horn_topological_2022}.
	\item Construct generalisable metrics to quantify various geometric and topological properties of GNNs and the datasets they are trained on.
	\item Construct a model, for instance, a transformer or CCNN~\cite{tdlbook}, that can learn topological features in data and benchmark against non-topological approaches.
\end{enumerate}

\section{Tree neighbours-match}
We decided to try to reproduce the over-squashing problem from \cite{alon_bottleneck_2021}.
\subsection{Data}
The dataset of a given depth $d$ is comprised of binary trees of depth $d$ and $n$ unique iid. trees. Each node of the graph has two features, a class, and the number of ``leaves''. We can represent a tree as a directed and connected graph $G \in (V, E)$ where $(i, j) \in E$ [COMMENT FROM RAGHAV?].
We let $A$ represent the adjacency matrix for a given tree without any self-loops. Let $A_{i,j}= \text{if } \lfloor \frac{i}{2}\rfloor = j \text{ then } 1 \text{ else } 0$. We use self-loops since they have a positive effect on performance [CITE THE CURVATURE PAPER(?)]: $\bar{A}=A+I$, where $I$ is the identity matrix.

The $V \in \mathbb{N}^{2 \times n}$ contain our attributes for the nodes. All nodes other than nodes at depth $d$ have a class of 0: $V_{1, i_{1}} = 0, i_{1} \in \{0, 1, 2, \ldots, n-2^{d}-1, n-2^{d}\}$. The remaining nodes are labelled in ascending order: $V_{1, i_{2}}= i_{2}-(n-2^{d}), i_{2} \in \{n-2^{d}+1, \ldots, n-1, n\}$.
The root has a random number of leaves between $1$ and $2^{d}$. The nodes in the last layer are sampled to have a random number of leaves between $1$ and $2^{d}$ without replacement. All nodes between depth $1$ to $d-1$ are set to have 0 leaves. The label for the dataset is then finally set to be the class whose leaves match the number of leaves of the root. Note that the edge matrix is 1-indexed and the attribute matrix is 0-indexed for ease of notation.

NOTE:
Our tree neighbours using the approximation fr birthday problem has almost 0 likelihood for collisions between test and train


\section{NOTES FROM RESEARCH THAT HAVE NOT BEEN ORGANISED!}

For each given depth $d$, we have $2^{d}! \cdot 2^{d}$ ($2^{d}!$ permutations of the bottom layer, $2^{d}$ possible root labels) possible trees / samples. We notice this means that for $d=2$ and $d=3$, we only get $96$ and $322560$ unique trees respectively.
We sample from this dataset by generating a binary tree, then creating a permutation of the unique number of leaves and then randomly picking a class that the root should mimic.

By the birthday paradox, we know this means these depths will very likely contain duplicate entries in the train and test data. Since they are both sampled IID this means it will not result in overfitting. In the event that the model has perfectly learnt the training data which contains all possible unique entries, we know that any future samples we throw at the model will also just contain the same entries we can classify.
For depths greater then 3, the number of unique trees grows to the point where the likelihood of duplicate entries goes towards 0.


\subsection{Running the experiment} % tree neighbours-match

Although in \cite{alon_bottleneck_2021}, the authors present their findings on the tree neighbours-match dataset, they do not provide results of implementing the fully adjacent last layer which they otherwise more widely propose as a heuristic approach to deal with over-squashing. 
This has led us to implement the fully adjacent last layer and compare the model with and without the last layer. We have in total run 1135 experiments, with a random distribution of parameters, to see how the GCN models with and without the fully adjacent last layer compare. The results are presented in \ref{fig:tree_experiment_graph}. More detailed results can be found \href{PLACE SOMETHING HERE MAYBE A LINK TO THE RESULTS IN THE}{here}.

\begin{figure}[H]
	\centering
	\input{tree_experiment_graph.tex}
	\caption{The results of the tree neighbours-match experiment.}
	\label{fig:tree_experiment_graph}
\end{figure}

\section{model archetecture} % tree neighbours-match

The two node features are embedded in a 32 dimensional space using a linear layer with trainable weights without a bias parameter.   

\subsection{Results} % tree neighbours-match

The results of the tree neighbours-match experiment show that contrary to the findings in \cite{alon_bottleneck_2021}, the fully adjacent last layer does not outperform the GCN model without the fully adjacent last layer. The results are consistent across all depths and the fully connected last layer outperforms the fully adjacent last layer in all cases. This is an interesting result and motivates further investigation into the over-squashing problem on a more theoretical level rather than just heuristic, as it is difficult to draw any conclusions from our contradictory results alone. 


\printbibliography

\end{document}

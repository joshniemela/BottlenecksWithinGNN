%This is a useful book about TDL
@book{tdlbook,
	title        = {Topological {Deep} {Learning}: {Going} {Beyond} {Graph} {Data}},
	shorttitle   = {Topological {Deep} {Learning}},
	author       = {Hajij, Mustafa  and Papamarkou, Theodore and Zamzmi, Ghada and Natesan  Ramamurthy, Karthikeyan and Birdal, Tolga and Schaub, Michael T.},
	year         = 2024,
	publisher    = {},
	url          = {https://tdlbook.org/},
	urldate      = {2024-08-19},
	abstract     = {A book on topological deep learning.}
}

@misc{wang_non-convolutional_2024,
	title        = {Non-convolutional {Graph} {Neural} {Networks}},
	author       = {Wang, Yuanqing and Cho, Kyunghyun},
	year         = 2024,
	month        = aug,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2408.00165},
	urldate      = {2024-08-19},
	note         = {arXiv:2408.00165 [cs]},
	abstract     = {Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence}
}

@article{ma_universal_2022,
	title        = {Universal {Graph} {Neural} {Networks} without {Message} {Passing}},
	author       = {Ma, George and Wang, Yifei and Wang, Yisen},
	year         = 2022,
	month        = sep,
	url          = {https://openreview.net/forum?id=P0bfBJaD4KP},
	urldate      = {2024-08-19},
	abstract     = {Message-Passing Graph Neural Networks (MP-GNNs) have become the de facto paradigm for learning on graph for years. Nevertheless, recent works also obtain promising empirical results with other kinds of architectures like global self-attention and even MLPs. This raises an important theoretical question: what is the minimal prerequisite for an expressive graph model? In this work, we theoretically show that when equipped with proper position encodings, even a simple Bag-of-Nodes (BoN) model (node-wise MLP followed by global readout) can be universal on graphs. We name this model as Universal Bag-of-Nodes (UBoN). Synthetic experiments on the EXP dataset show that UBoN indeed achieves expressive power beyond 1-WL test. On real-world graph classification tasks, UBoN also obtains comparable performance to MP-GNNs while enjoying better training and inference efficiency (50\% less training time compared to GCN). We believe that our theoretical and empirical results might inspire more research on simple and expressive GNN architectures.},
	language     = {en}
}

@misc{di_giovanni_over-squashing_2023,
	title        = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	shorttitle   = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}},
	author       = {Di Giovanni, Francesco and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio', Pietro and Bronstein, Michael},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2302.02941},
	urldate      = {2024-08-19},
	note         = {arXiv:2302.02941 [cs, stat]},
	abstract     = {Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Discrete Mathematics, Statistics - Machine Learning}
}

This is a meta-analysis of the field of topological deep learning. It is a good starting point for understanding the field.  
Section 6.1 talks about the advantages of topological deep learning and is a section Raghav is interested in exploring further.
@misc{papamarkou_position:_2024,
	title        = {Position: {Topological} {Deep} {Learning} is the {New} {Frontier} for {Relational} {Learning}},
	shorttitle   = {Position},
	author       = {Papamarkou, Theodore and Birdal, Tolga and Bronstein, Michael and Carlsson, Gunnar and Curry, Justin and Gao, Yue and Hajij, Mustafa and Kwitt, Roland and Liò, Pietro and Di Lorenzo, Paolo and Maroulas, Vasileios and Miolane, Nina and Nasrin, Farzana and Ramamurthy, Karthikeyan Natesan and Rieck, Bastian and Scardapane, Simone and Schaub, Michael T. and Veličković, Petar and Wang, Bei and Wang, Yusu and Wei, Guo-Wei and Zamzmi, Ghada},
	year         = 2024,
	month        = aug,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2402.08871},
	urldate      = {2024-08-19},
	note         = {arXiv:2402.08871 [cs, stat]},
	abstract     = {Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL is the new frontier for relational learning. TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

This paper talks about the oversquashing problem and long-range interactions in GNNs. Networks such as GCN and GIN that do not differentiate between messages perform worse than GATs and GGNN.  
SOTA models were beaten by adding a trivial fully-adjacent graph layer to the end of the GNN.  
The authors also made a video explaining the paper: [YouTube](https://www.youtube.com/watch?v=XQHfkHTAo0s)  
This was a source suggested by Raghav.
@misc{alon_bottleneck_2021,
	title        = {On the {Bottleneck} of {Graph} {Neural} {Networks} and its {Practical} {Implications}},
	author       = {Alon, Uri and Yahav, Eran},
	year         = 2021,
	month        = mar,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2006.05205},
	urldate      = {2024-08-19},
	note         = {arXiv:2006.05205 [cs, stat]},
	abstract     = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

This was a source suggested by Raghav.
@misc{horn_topological_2022,
	title        = {Topological {Graph} {Neural} {Networks}},
	author       = {Horn, Max and De Brouwer, Edward and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
	year         = 2022,
	month        = mar,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2102.07835},
	urldate      = {2024-08-19},
	note         = {arXiv:2102.07835 [cs, math, stat]},
	abstract     = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.},
	keywords     = {Computer Science - Machine Learning, Mathematics - Algebraic Topology, Statistics - Machine Learning}
}

The authors found out that over-squashing is a different problem from "under-reaching", which is that the network isn't large enough to be able to make a node reach another node. They also reproduce the results of adding a FA making all GNNs better. They found out that the number of edges added from the FA also correlates with a reduction in errors even if it isn't the entire layer. Hidden sizes are much less relevant than just adding the FA.
@misc{topping_understanding_2022,
	title        = {Understanding over-squashing and bottlenecks on graphs via curvature},
	author       = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	year         = 2022,
	month        = nov,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2111.14522},
	urldate      = {2024-08-19},
	note         = {arXiv:2111.14522 [cs, stat]},
	abstract     = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
	keywords     = {Statistics - Machine Learning, Computer Science - Machine Learning}
}

@misc{choi_topology-informed_2024,
	title        = {Topology-{Informed} {Graph} {Transformer}},
	author       = {Choi, Yun Young and Park, Sun Woo and Lee, Minho and Woo, Youngho},
	year         = 2024,
	month        = feb,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2402.02005},
	urldate      = {2024-08-19},
	note         = {arXiv:2402.02005 [cs]},
	abstract     = {Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for better feature representation. TIGT outperforms previous Graph Transformers in classifying synthetic dataset aimed at distinguishing isomorphism classes of graphs. Additionally, mathematical analysis and empirical evaluations highlight our model's competitive edge over state-of-the-art Graph Transformers across various benchmark datasets.},
	keywords     = {Computer Science - Machine Learning}
}

@article{zhong_hierarchical_2023,
	title        = {Hierarchical message-passing graph neural networks},
	author       = {Zhong, Zhiqiang and Li, Cheng-Te and Pang, Jun},
	year         = 2023,
	month        = jan,
	journal      = {Data Mining and Knowledge Discovery},
	volume       = 37,
	number       = 1,
	pages        = {381--408},
	doi          = {10.1007/s10618-022-00890-9},
	issn         = {1573-756X},
	url          = {https://doi.org/10.1007/s10618-022-00890-9},
	urldate      = {2024-08-19},
	abstract     = {Graph Neural Networks (GNNs) have become a prominent approach to machine learning with graphs and have been increasingly applied in a multitude of domains. Nevertheless, since most existing GNN models are based on flat message-passing mechanisms, two limitations need to be tackled: (i) they are costly in encoding long-range information spanning the graph structure; (ii) they are failing to encode features in the high-order neighbourhood in the graphs as they only perform information aggregation across the observed edges in the original graph. To deal with these two issues, we propose a novel Hierarchical Message-passing Graph Neural Networks framework. The key idea is generating a hierarchical structure that re-organises all nodes in a flat graph into multi-level super graphs, along with innovative intra- and inter-level propagation manners. The derived hierarchy creates shortcuts connecting far-away nodes so that informative long-range interactions can be efficiently accessed via message passing and incorporates meso- and macro-level semantics into the learned node representations. We present the first model to implement this framework, termed Hierarchical Community-aware Graph Neural Network (HC-GNN), with the assistance of a hierarchical community detection algorithm. The theoretical analysis illustrates HC-GNN’s remarkable capacity in capturing long-range information without introducing heavy additional computation complexity. Empirical experiments conducted on 9 datasets under transductive, inductive, and few-shot settings exhibit that HC-GNN can outperform state-of-the-art GNN models in network analysis tasks, including node classification, link prediction, and community detection. Moreover, the model analysis further demonstrates HC-GNN’s robustness facing graph sparsity and the flexibility in incorporating different GNN encoders.},
	language     = {en},
	keywords     = {Artificial Intelligence , Graph neural networks, Hierarchical message-passing, Long range communication, Hierarchical structure, Representation learning}
}

@inproceedings{tortorella_leave_2022,
	title        = {Leave {Graphs} {Alone}: {Addressing} {Over}-{Squashing} without {Rewiring}},
	shorttitle   = {Leave {Graphs} {Alone}},
	author       = {Tortorella, Domenico and Micheli, Alessio},
	year         = 2022,
	month        = nov,
	url          = {https://openreview.net/forum?id=vEbUaN9Z2V8},
	urldate      = {2024-08-19},
	abstract     = {Recent works have investigated the role of graph bottlenecks in preventing long-range information propagation in message-passing graph neural networks, causing the so-called `over-squashing' phenomenon. As a remedy, graph rewiring mechanisms have been proposed as preprocessing steps. Graph Echo State Networks (GESNs) are a reservoir computing model for graphs, where node embeddings are recursively computed by an untrained message-passing function. In this paper, we show that GESNs can achieve a significantly better accuracy on six heterophilic node classification tasks without altering the graph connectivity, thus suggesting a different route for addressing the over-squashing problem.},
	language     = {en}
}

@misc{black_understanding_2023,
	title        = {Understanding {Oversquashing} in {GNNs} through the {Lens} of {Effective} {Resistance}},
	author       = {Black, Mitchell and Wan, Zhengchao and Nayyeri, Amir and Wang, Yusu},
	year         = 2023,
	month        = jun,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2302.06835},
	urldate      = {2024-08-19},
	note         = {arXiv:2302.06835 [cs]},
	abstract     = {Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the ``strength'' of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.},
	keywords     = {Computer Science - Machine Learning}
}

@misc{ying_hierarchical_2019,
	title        = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	author       = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	year         = 2019,
	month        = feb,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/1806.08804},
	urldate      = {2024-08-19},
	note         = {arXiv:1806.08804 [cs, stat]},
	abstract     = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks, Statistics - Machine Learning}
}

@misc{sobolevsky_hierarchical_2021,
	title        = {Hierarchical {Graph} {Neural} {Networks}},
	author       = {Sobolevsky, Stanislav},
	year         = 2021,
	month        = may,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2105.03388},
	urldate      = {2024-08-19},
	note         = {arXiv:2105.03388 [physics]},
	abstract     = {Over the recent years, Graph Neural Networks have become increasingly popular in network analytic and beyond. With that, their architecture noticeable diverges from the classical multi-layered hierarchical organization of the traditional neural networks. At the same time, many conventional approaches in network science efficiently utilize the hierarchical approaches to account for the hierarchical organization of the networks, and recent works emphasize their critical importance. This paper aims to connect the dots between the traditional Neural Network and the Graph Neural Network architectures as well as the network science approaches, harnessing the power of the hierarchical network organization. A Hierarchical Graph Neural Network architecture is proposed, supplementing the original input network layer with the hierarchy of auxiliary network layers and organizing the computational scheme updating the node features through both - horizontal network connections within each layer as well as the vertical connection between the layers. It enables simultaneous learning of the individual node features along with the aggregated network features at variable resolution and uses them to improve the convergence and stability of the individual node feature learning. The proposed Hierarchical Graph Neural network architecture is successfully evaluated on the network embedding and modeling as well as network classification, node labeling, and community tasks and demonstrates increased efficiency in those.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Combinatorics, Physics - Data Analysis, Statistics and Probability, 68T07, 05C85}
}

@inproceedings{giraldo_trade-off_2023,
	title        = {On the {Trade}-off between {Over}-smoothing and {Over}-squashing in {Deep} {Graph} {Neural} {Networks}},
	author       = {Giraldo, Jhony H. and Skianis, Konstantinos and Bouwmans, Thierry and Malliaros, Fragkiskos D.},
	year         = 2023,
	month        = oct,
	booktitle    = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	pages        = {566--576},
	doi          = {10.1145/3583780.3614997},
	url          = {http://arxiv.org/abs/2212.02374},
	urldate      = {2024-08-19},
	note         = {arXiv:2212.02374 [cs]},
	abstract     = {Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining the graph unchanged during testing. Comprehensive comparisons demonstrate SJLR's competitive performance in addressing over-smoothing and over-squashing.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence}
}

@misc{buterez_masked_2024,
	title        = {Masked {Attention} is {All} {You} {Need} for {Graphs}},
	author       = {Buterez, David and Janet, Jon Paul and Oglic, Dino and Lio, Pietro},
	year         = 2024,
	month        = feb,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2402.10793},
	urldate      = {2024-08-19},
	note         = {arXiv:2402.10793 [cs]},
	abstract     = {Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly better transfer learning capabilities compared to GNNs and comparable or better time and memory scaling. MAG has sub-linear memory scaling in the number of nodes or edges, enabling learning on dense graphs and future-proofing the approach.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence}
}

@misc{li_deeper_2018,
	title        = {Deeper {Insights} into {Graph} {Convolutional} {Networks} for {Semi}-{Supervised} {Learning}},
	author       = {Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
	year         = 2018,
	month        = jan,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1801.07606},
	url          = {http://arxiv.org/abs/1801.07606},
	urldate      = {2024-08-19},
	note         = {arXiv:1801.07606 [cs, stat]},
	abstract     = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires a considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

% Sources for QM9
@article{blum,
	title        = {970 Million Druglike Small Molecules for Virtual Screening in the Chemical Universe Database {GDB-13}},
	author       = {L. C. Blum and J.-L. Reymond},
	year         = 2009,
	journal      = {J. Am. Chem. Soc.},
	volume       = 131,
	pages        = 8732
}

@article{rupp,
	title        = {Fast and accurate modeling of molecular atomization energies with machine learning},
	author       = {M. Rupp and A. Tkatchenko and K.-R. M\"uller and O. A. von Lilienfeld},
	year         = 2012,
	journal      = {Physical Review Letters},
	volume       = 108,
	pages        = 058301
}

@article{hamiltonYL17,
  author       = {William L. Hamilton and
                  Rex Ying and
                  Jure Leskovec},
  title        = {Inductive Representation Learning on Large Graphs},
  journal      = {CoRR},
  volume       = {abs/1706.02216},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.02216},
  eprinttype    = {arXiv},
  eprint       = {1706.02216},
  timestamp    = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HamiltonYL17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yang2023graphneuralnetworksinherently,
      title={Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs}, 
      author={Chenxiao Yang and Qitian Wu and Jiahua Wang and Junchi Yan},
      year={2023},
      eprint={2212.09034},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.09034}, 
}

@BOOK{Cormen2009-aq,
  title     = "Introduction to Algorithms",
  author    = "Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L
               and Stein, Clifford",
  publisher = "MIT Press",
  series    = "The MIT Press",
  edition   =  3,
  month     =  jul,
  year      =  2009,
  address   = "London, England"
}

@misc{rusch2023surveyoversmoothinggraphneural,
      title={A Survey on Oversmoothing in Graph Neural Networks}, 
      author={T. Konstantin Rusch and Michael M. Bronstein and Siddhartha Mishra},
      year={2023},
      eprint={2303.10993},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.10993}, 
}

@misc{wu2023nonasymptoticanalysisoversmoothinggraph,
      title={A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks}, 
      author={Xinyi Wu and Zhengdao Chen and William Wang and Ali Jadbabaie},
      year={2023},
      eprint={2212.10701},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.10701}, 
}

@misc{ronneberger2015unetconvolutionalnetworksbiomedical,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.04597}, 
}

@misc{hofer2021graphfiltrationlearning,
      title={Graph Filtration Learning}, 
      author={Christoph D. Hofer and Florian Graf and Bastian Rieck and Marc Niethammer and Roland Kwitt},
      year={2021},
      eprint={1905.10996},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.10996}, 
}

@misc{buterez2022graphneuralnetworksadaptive,
      title={Graph Neural Networks with Adaptive Readouts}, 
      author={David Buterez and Jon Paul Janet and Steven J. Kiddle and Dino Oglic and Pietro Liò},
      year={2022},
      eprint={2211.04952},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.04952}, 
}

@article{Gundersen_Kjensmo_2018, title={State of the Art: Reproducibility in Artificial Intelligence}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11503}, DOI={10.1609/aaai.v32i1.11503}, abstractNote={ &lt;p&gt; Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gundersen, Odd Erik and Kjensmo, Sigbjørn}, year={2018}, month={Apr.} }
This is a useful book about TDL
@book{tdlbook,
	title = {Topological {Deep} {Learning}: {Going} {Beyond} {Graph} {Data}},
	shorttitle = {Topological {Deep} {Learning}},
	url = {https://tdlbook.org/},
	abstract = {A book on topological deep learning.},
	urldate = {2024-08-19},
	author = {Hajij, Mustafa  and Papamarkou, Theodore and Zamzmi, Ghada and Natesan  Ramamurthy, Karthikeyan and Birdal, Tolga and Schaub, Michael T.},
    publisher = {},
    year = {2024},
}


@misc{wang_non-convolutional_2024,
	title = {Non-convolutional {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2408.00165},
	abstract = {Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Wang, Yuanqing and Cho, Kyunghyun},
	month = aug,
	year = {2024},
	note = {arXiv:2408.00165 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{ma_universal_2022,
	title = {Universal {Graph} {Neural} {Networks} without {Message} {Passing}},
	url = {https://openreview.net/forum?id=P0bfBJaD4KP},
	abstract = {Message-Passing Graph Neural Networks (MP-GNNs) have become the de facto paradigm for learning on graph for years. Nevertheless, recent works also obtain promising empirical results with other kinds of architectures like global self-attention and even MLPs. This raises an important theoretical question: what is the minimal prerequisite for an expressive graph model? In this work, we theoretically show that when equipped with proper position encodings, even a simple Bag-of-Nodes (BoN) model (node-wise MLP followed by global readout) can be universal on graphs. We name this model as Universal Bag-of-Nodes (UBoN). Synthetic experiments on the EXP dataset show that UBoN indeed achieves expressive power beyond 1-WL test. On real-world graph classification tasks, UBoN also obtains comparable performance to MP-GNNs while enjoying better training and inference efficiency (50\% less training time compared to GCN). We believe that our theoretical and empirical results might inspire more research on simple and expressive GNN architectures.},
	language = {en},
	urldate = {2024-08-19},
	author = {Ma, George and Wang, Yifei and Wang, Yisen},
	month = sep,
	year = {2022},
}

@misc{di_giovanni_over-squashing_2023,
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	shorttitle = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2302.02941},
	abstract = {Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Di Giovanni, Francesco and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio', Pietro and Bronstein, Michael},
	month = may,
	year = {2023},
	note = {arXiv:2302.02941 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Discrete Mathematics, Statistics - Machine Learning},
}

This is a meta-analysis of the field of topological deep learning. It is a good starting point for understanding the field.  
Section 6.1 talks about the advantages of topological deep learning and is a section Raghav is interested in exploring further.
@misc{papamarkou_position:_2024,
	title = {Position: {Topological} {Deep} {Learning} is the {New} {Frontier} for {Relational} {Learning}},
	shorttitle = {Position},
	url = {http://arxiv.org/abs/2402.08871},
	abstract = {Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL is the new frontier for relational learning. TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Papamarkou, Theodore and Birdal, Tolga and Bronstein, Michael and Carlsson, Gunnar and Curry, Justin and Gao, Yue and Hajij, Mustafa and Kwitt, Roland and Liò, Pietro and Di Lorenzo, Paolo and Maroulas, Vasileios and Miolane, Nina and Nasrin, Farzana and Ramamurthy, Karthikeyan Natesan and Rieck, Bastian and Scardapane, Simone and Schaub, Michael T. and Veličković, Petar and Wang, Bei and Wang, Yusu and Wei, Guo-Wei and Zamzmi, Ghada},
	month = aug,
	year = {2024},
	note = {arXiv:2402.08871 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

This paper talks about the oversquashing problem and long-range interactions in GNNs. Networks such as GCN and GIN that do not differentiate between messages perform worse than GATs and GGNN.  
SOTA models were beaten by adding a trivial fully-adjacent graph layer to the end of the GNN.  
The authors also made a video explaining the paper: [YouTube](https://www.youtube.com/watch?v=XQHfkHTAo0s)  
This was a source suggested by Raghav.
@misc{alon_bottleneck_2021,
	title = {On the {Bottleneck} of {Graph} {Neural} {Networks} and its {Practical} {Implications}},
	url = {http://arxiv.org/abs/2006.05205},
	abstract = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Alon, Uri and Yahav, Eran},
	month = mar,
	year = {2021},
	note = {arXiv:2006.05205 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

This was a source suggested by Raghav.
@misc{horn_topological_2022,
	title = {Topological {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2102.07835},
	abstract = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Horn, Max and De Brouwer, Edward and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
	month = mar,
	year = {2022},
	note = {arXiv:2102.07835 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Algebraic Topology, Statistics - Machine Learning},
}

The authors found out that over-squashing is a different problem from "under-reaching", which is that the network isn't large enough to be able to make a node reach another node. They also reproduce the results of adding a FA making all GNNs better. They found out that the number of edges added from the FA also correlates with a reduction in errors even if it isn't the entire layer. Hidden sizes are much less relevant than just adding the FA.
@misc{topping_understanding_2022,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {http://arxiv.org/abs/2111.14522},
	abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	month = nov,
	year = {2022},
	note = {arXiv:2111.14522 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{choi_topology-informed_2024,
	title = {Topology-{Informed} {Graph} {Transformer}},
	url = {http://arxiv.org/abs/2402.02005},
	abstract = {Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for better feature representation. TIGT outperforms previous Graph Transformers in classifying synthetic dataset aimed at distinguishing isomorphism classes of graphs. Additionally, mathematical analysis and empirical evaluations highlight our model's competitive edge over state-of-the-art Graph Transformers across various benchmark datasets.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Choi, Yun Young and Park, Sun Woo and Lee, Minho and Woo, Youngho},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02005 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{zhong_hierarchical_2023,
	title = {Hierarchical message-passing graph neural networks},
	volume = {37},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-022-00890-9},
	doi = {10.1007/s10618-022-00890-9},
	abstract = {Graph Neural Networks (GNNs) have become a prominent approach to machine learning with graphs and have been increasingly applied in a multitude of domains. Nevertheless, since most existing GNN models are based on flat message-passing mechanisms, two limitations need to be tackled: (i) they are costly in encoding long-range information spanning the graph structure; (ii) they are failing to encode features in the high-order neighbourhood in the graphs as they only perform information aggregation across the observed edges in the original graph. To deal with these two issues, we propose a novel Hierarchical Message-passing Graph Neural Networks framework. The key idea is generating a hierarchical structure that re-organises all nodes in a flat graph into multi-level super graphs, along with innovative intra- and inter-level propagation manners. The derived hierarchy creates shortcuts connecting far-away nodes so that informative long-range interactions can be efficiently accessed via message passing and incorporates meso- and macro-level semantics into the learned node representations. We present the first model to implement this framework, termed Hierarchical Community-aware Graph Neural Network (HC-GNN), with the assistance of a hierarchical community detection algorithm. The theoretical analysis illustrates HC-GNN’s remarkable capacity in capturing long-range information without introducing heavy additional computation complexity. Empirical experiments conducted on 9 datasets under transductive, inductive, and few-shot settings exhibit that HC-GNN can outperform state-of-the-art GNN models in network analysis tasks, including node classification, link prediction, and community detection. Moreover, the model analysis further demonstrates HC-GNN’s robustness facing graph sparsity and the flexibility in incorporating different GNN encoders.},
	language = {en},
	number = {1},
	urldate = {2024-08-19},
	journal = {Data Mining and Knowledge Discovery},
	author = {Zhong, Zhiqiang and Li, Cheng-Te and Pang, Jun},
	month = jan,
	year = {2023},
	keywords = {
                Artificial Intelligence
            , Graph neural networks, Hierarchical message-passing, Long range communication, Hierarchical structure, Representation learning},
	pages = {381--408},
}

@inproceedings{tortorella_leave_2022,
	title = {Leave {Graphs} {Alone}: {Addressing} {Over}-{Squashing} without {Rewiring}},
	shorttitle = {Leave {Graphs} {Alone}},
	url = {https://openreview.net/forum?id=vEbUaN9Z2V8},
	abstract = {Recent works have investigated the role of graph bottlenecks in preventing long-range information propagation in message-passing graph neural networks, causing the so-called `over-squashing' phenomenon. As a remedy, graph rewiring mechanisms have been proposed as preprocessing steps. Graph Echo State Networks (GESNs) are a reservoir computing model for graphs, where node embeddings are recursively computed by an untrained message-passing function. In this paper, we show that GESNs can achieve a significantly better accuracy on six heterophilic node classification tasks without altering the graph connectivity, thus suggesting a different route for addressing the over-squashing problem.},
	language = {en},
	urldate = {2024-08-19},
	author = {Tortorella, Domenico and Micheli, Alessio},
	month = nov,
	year = {2022},
}

@misc{black_understanding_2023,
	title = {Understanding {Oversquashing} in {GNNs} through the {Lens} of {Effective} {Resistance}},
	url = {http://arxiv.org/abs/2302.06835},
	abstract = {Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the ``strength'' of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Black, Mitchell and Wan, Zhengchao and Nayyeri, Amir and Wang, Yusu},
	month = jun,
	year = {2023},
	note = {arXiv:2302.06835 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ying_hierarchical_2019,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	url = {http://arxiv.org/abs/1806.08804},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	month = feb,
	year = {2019},
	note = {arXiv:1806.08804 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@misc{sobolevsky_hierarchical_2021,
	title = {Hierarchical {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2105.03388},
	abstract = {Over the recent years, Graph Neural Networks have become increasingly popular in network analytic and beyond. With that, their architecture noticeable diverges from the classical multi-layered hierarchical organization of the traditional neural networks. At the same time, many conventional approaches in network science efficiently utilize the hierarchical approaches to account for the hierarchical organization of the networks, and recent works emphasize their critical importance. This paper aims to connect the dots between the traditional Neural Network and the Graph Neural Network architectures as well as the network science approaches, harnessing the power of the hierarchical network organization. A Hierarchical Graph Neural Network architecture is proposed, supplementing the original input network layer with the hierarchy of auxiliary network layers and organizing the computational scheme updating the node features through both - horizontal network connections within each layer as well as the vertical connection between the layers. It enables simultaneous learning of the individual node features along with the aggregated network features at variable resolution and uses them to improve the convergence and stability of the individual node feature learning. The proposed Hierarchical Graph Neural network architecture is successfully evaluated on the network embedding and modeling as well as network classification, node labeling, and community tasks and demonstrates increased efficiency in those.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Sobolevsky, Stanislav},
	month = may,
	year = {2021},
	note = {arXiv:2105.03388 [physics]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Combinatorics, Physics - Data Analysis, Statistics and Probability, 68T07, 05C85},
}

@inproceedings{giraldo_trade-off_2023,
	title = {On the {Trade}-off between {Over}-smoothing and {Over}-squashing in {Deep} {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2212.02374},
	doi = {10.1145/3583780.3614997},
	abstract = {Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining the graph unchanged during testing. Comprehensive comparisons demonstrate SJLR's competitive performance in addressing over-smoothing and over-squashing.},
	urldate = {2024-08-19},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Giraldo, Jhony H. and Skianis, Konstantinos and Bouwmans, Thierry and Malliaros, Fragkiskos D.},
	month = oct,
	year = {2023},
	note = {arXiv:2212.02374 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {566--576},
}

@misc{papamarkou_position:_2024-1,
	title = {Position: {Topological} {Deep} {Learning} is the {New} {Frontier} for {Relational} {Learning}},
	shorttitle = {Position},
	url = {http://arxiv.org/abs/2402.08871},
	abstract = {Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL is the new frontier for relational learning. TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Papamarkou, Theodore and Birdal, Tolga and Bronstein, Michael and Carlsson, Gunnar and Curry, Justin and Gao, Yue and Hajij, Mustafa and Kwitt, Roland and Liò, Pietro and Di Lorenzo, Paolo and Maroulas, Vasileios and Miolane, Nina and Nasrin, Farzana and Ramamurthy, Karthikeyan Natesan and Rieck, Bastian and Scardapane, Simone and Schaub, Michael T. and Veličković, Petar and Wang, Bei and Wang, Yusu and Wei, Guo-Wei and Zamzmi, Ghada},
	month = aug,
	year = {2024},
	note = {arXiv:2402.08871 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{buterez_masked_2024,
	title = {Masked {Attention} is {All} {You} {Need} for {Graphs}},
	url = {http://arxiv.org/abs/2402.10793},
	abstract = {Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly better transfer learning capabilities compared to GNNs and comparable or better time and memory scaling. MAG has sub-linear memory scaling in the number of nodes or edges, enabling learning on dense graphs and future-proofing the approach.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Buterez, David and Janet, Jon Paul and Oglic, Dino and Lio, Pietro},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10793 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}